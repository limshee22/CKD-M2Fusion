# CKD‑M2Fusion: Multi‑Modal Transformer Framework for Chronic Kidney Disease Staging via Serum Creatinine Regression

CKD‑M2Fusion fuses structured clinical data with *kidney‑focused CT images* to regress baseline serum creatinine (baseline_cr).
The framework fuses **TabPFN** embeddings of handcrafted clinic/kidney features with **Radiology‑Fortified DINOv2 ViT‑Base** image representations, enabling end‑to‑end multi‑modal learning that is data‑efficient and readily extensible.

---

### 🔧 Model Overview
1. **Kidney CT Branch**
   - Performs kidney segmentation on axial CT slices, producing binary masks.  
   - Retains only slices containing kidney tissue (mask > 0).  
   - Each selected slice is passed through a DINOv2 ViT‑Base encoder fine‑tuned on our dataset (weights initialized from AMOS‑pretrained model).

2. **Clinical Tabular Branch**
   - Hand‑crafted demographic, laboratory, and kidney‑specific variables are fed into a **TabPFN** transformer encoder.  
   - TabPFN is trained in a few‑shot fashion to embed tabular features into a latent vector.

3. **Feature Fusion & Prediction**
   - Slice‑wise visual features are concatenated (or mean‑pooled) to form a single visual descriptor per patient.  
   - The visual descriptor is concatenated with the TabPFN clinical embedding.  
   - A lightweight MLP head outputs the regressed baseline_cr (mg/dL).

---

### ⚙️ Key Features
1. **Dual Pre‑trained Backbones**  
   - *DINOv2 ViT‑Base* initialized from AMOS weights for robust medical imaging performance.  
   - *TabPFN* leverages large‑scale synthetic pre‑training for rapid convergence on small clinical datasets.

2. **Segmentation‑Aware Slice Selection**  
   - Automatically filters out non‑kidney slices, reducing noise and computation.

3. **Customizable Slice Resampling**  
   - Optional resampling to a fixed number of slices per study ensures consistent input shapes.

4. **End‑to‑End Multi‑Modal Learning**  
   - Single optimizer updates both branches and the fusion head, preserving modality‑specific nuances while learning synergistic representations.

---

### 🧾 Dataloader Structure
| Component | Format | Description |
|-----------|--------|-------------|
| **Images** | `(B, S, H, W)` | `B`: batch, `S`: selected slices, `H×W`: 224×224 (after resample) |
| **Clinical Features** | `(B, F)` | `F`: number of handcrafted variables |
| **Targets** | `(B,)` / `(B, 1)` | baseline_cr |

1. Within each batch, CT slices are sorted by anatomical order.  
2. For every patient, *per‑slice* features `v₁…v_S` are extracted, concatenated → `V_patient`.  
3. Clinical embedding `C_patient` is generated by TabPFN.  
4. Final feature vector: `[V_patient ‖ C_patient]` → MLP → outputs.

---

### 📁 Example Workflow
```text
Batch 1:
- Patient A:
    · CT Slices: [slice_01 … slice_N]  ➜  ViT Features: [A1 … AN]
    · Clinical Features: tab_A
    · Fusion: concat([A1 … AN] , tab_A) ➜ MLP ➜ baseline_cr_A

- Patient B: ...
```
---

## ⚙️ Installation
```text
git clone https://github.com/limshee22/CKD-M2Fusion.git
pip install -r requirements.txt
```

---

## 🚀 Starting
```text
python main.py \
    --train_csv ./Data/train.csv \
    --test_csv ./Data/test.csv \
    --pretrained_weights model_final.pth \
    --batch_size 32 \
    --target_slices 32 \
    --epochs 100 \
    --lr 1e-4 \
    --weight_decay 1e-4 \
    --early_stopping 10 \
    --log_interval 10 \
    --checkpoint_dir ./checkpoint \
    --log_dir ./logs \
    --results_dir ./result \
    --seed 42 \
    --tabpfn_dim 64 \
```




## 📚 Citing
If you use this repository in your work, please consider citing the following.

You can download code about RadioloDINOv2ForRadiology [here](https://github.com/MohammedSB/DINOv2ForRadiology/tree/main).
```text
@misc{baharoon2023general,
      title={Towards General Purpose Vision Foundation Models for Medical Image Analysis: An Experimental Study of DINOv2 on Radiology Benchmarks}, 
      author={Mohammed Baharoon and Waseem Qureshi and Jiahong Ouyang and Yanwu Xu and Abdulrhman Aljouie and Wei Peng},
      year={2023},
      eprint={2312.02366},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{oquab2023dinov2,
      title={DINOv2: Learning Robust Visual Features without Supervision}, 
      author={Maxime Oquab and Timothée Darcet and Théo Moutakanni and Huy Vo and Marc Szafraniec and Vasil Khalidov and Pierre Fernandez and Daniel Haziza and Francisco Massa and Alaaeldin El-Nouby and Mahmoud Assran and Nicolas Ballas and Wojciech Galuba and Russell Howes and Po-Yao Huang and Shang-Wen Li and Ishan Misra and Michael Rabbat and Vasu Sharma and Gabriel Synnaeve and Hu Xu and Hervé Jegou and Julien Mairal and Patrick Labatut and Armand Joulin and Piotr Bojanowski},
      year={2023},
      eprint={2304.07193},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
```

```text
You can read our paper explaining TabPFN [here](https://www.nature.com/articles/s41586-024-08328-6).
@article{hollmann2025tabpfn,
 title={Accurate predictions on small data with a tabular foundation model},
 author={Hollmann, Noah and M{\"u}ller, Samuel and Purucker, Lennart and
         Krishnakumar, Arjun and K{\"o}rfer, Max and Hoo, Shi Bin and
         Schirrmeister, Robin Tibor and Hutter, Frank},
 journal={Nature},
 year={2025},
 month={01},
 day={09},
 doi={10.1038/s41586-024-08328-6},
 publisher={Springer Nature},
 url={https://www.nature.com/articles/s41586-024-08328-6},
}

@inproceedings{hollmann2023tabpfn,
  title={TabPFN: A transformer that solves small tabular classification problems in a second},
  author={Hollmann, Noah and M{\"u}ller, Samuel and Eggensperger, Katharina and Hutter, Frank},
  booktitle={International Conference on Learning Representations 2023},
  year={2023}
}
```