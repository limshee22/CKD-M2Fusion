# CKDâ€‘M2Fusion: Multiâ€‘Modal Transformer Framework for Chronic Kidney Disease Staging via Serum Creatinine Regression

CKDâ€‘M2Fusion fuses structured clinical data with *kidneyâ€‘focused CT images* to regress baseline serum creatinineâ€¯(baseline_cr).
The framework fuses **TabPFN** embeddings of handcrafted clinic/kidney features with **Radiologyâ€‘Fortified DINOv2 ViTâ€‘Base** image representations, enabling endâ€‘toâ€‘end multiâ€‘modal learning that is dataâ€‘efficient and readily extensible.

---

### ğŸ”§ Model Overview
1. **Kidney CT Branch**
   - Performs kidney segmentation on axial CT slices, producing binary masks.  
   - Retains only slices containing kidney tissue (mask >â€¯0).  
   - Each selected slice is passed through a DINOv2 ViTâ€‘Base encoder fineâ€‘tuned on our dataset (weights initialized from AMOSâ€‘pretrained model).

2. **Clinical Tabular Branch**
   - Handâ€‘crafted demographic, laboratory, and kidneyâ€‘specific variables are fed into a **TabPFN** transformer encoder.  
   - TabPFN is trained in a fewâ€‘shot fashion to embed tabular features into a latent vector.

3. **Feature Fusion & Prediction**
   - Sliceâ€‘wise visual features are concatenated (or meanâ€‘pooled) to form a single visual descriptor per patient.  
   - The visual descriptor is concatenated with the TabPFN clinical embedding.  
   - A lightweight MLP head outputs the regressed baseline_crâ€¯(mg/dL).

---

### âš™ï¸ Key Features
1. **Dual Preâ€‘trained Backbones**  
   - *DINOv2 ViTâ€‘Base* initialized from AMOS weights for robust medical imaging performance.  
   - *TabPFN* leverages largeâ€‘scale synthetic preâ€‘training for rapid convergence on small clinical datasets.

2. **Segmentationâ€‘Aware Slice Selection**  
   - Automatically filters out nonâ€‘kidney slices, reducing noise and computation.

3. **Customizable Slice Resampling**  
   - Optional resampling to a fixed number of slices per study ensures consistent input shapes.

4. **Endâ€‘toâ€‘End Multiâ€‘Modal Learning**  
   - Single optimizer updates both branches and the fusion head, preserving modalityâ€‘specific nuances while learning synergistic representations.

---

### ğŸ§¾ Dataloader Structure
| Component | Format | Description |
|-----------|--------|-------------|
| **Images** | `(B,â€¯S,â€¯H,â€¯W)` | `B`: batch, `S`: selected slices, `HÃ—W`: 224Ã—224 (after resample) |
| **Clinical Features** | `(B,â€¯F)` | `F`: number of handcrafted variables |
| **Targets** | `(B,)` / `(B,â€¯1)` | baseline_cr |

1. Within each batch, CT slices are sorted by anatomical order.  
2. For every patient, *perâ€‘slice* features `vâ‚â€¦v_S` are extracted, concatenated â†’ `V_patient`.  
3. Clinical embedding `C_patient` is generated by TabPFN.  
4. Final feature vector: `[V_patient â€– C_patient]` â†’ MLP â†’ outputs.

---

### ğŸ“ Example Workflow
```text
Batch 1:
- Patient A:
    Â· CT Slices: [slice_01 â€¦ slice_N]  âœ  ViT Features: [A1 â€¦ AN]
    Â· Clinical Features: tab_A
    Â· Fusion: concat([A1 â€¦ AN] , tab_A) âœ MLP âœ baseline_cr_A

- Patient B: ...
```
---

## âš™ï¸ Installation
```text
git clone https://github.com/limshee22/CKD-M2Fusion.git
pip install -r requirements.txt
```

---

## ğŸš€ Starting
```text
python main.py \
    --train_csv ./Data/train.csv \
    --test_csv ./Data/test.csv \
    --pretrained_weights model_final.pth \
    --batch_size 32 \
    --target_slices 32 \
    --epochs 100 \
    --lr 1e-4 \
    --weight_decay 1e-4 \
    --early_stopping 10 \
    --log_interval 10 \
    --checkpoint_dir ./checkpoint \
    --log_dir ./logs \
    --results_dir ./result \
    --seed 42 \
    --tabpfn_dim 64 \
```




## ğŸ“š Citing
If you use this repository in your work, please consider citing the following.

You can download code about RadioloDINOv2ForRadiology [here](https://github.com/MohammedSB/DINOv2ForRadiology/tree/main).
```text
@misc{baharoon2023general,
      title={Towards General Purpose Vision Foundation Models for Medical Image Analysis: An Experimental Study of DINOv2 on Radiology Benchmarks}, 
      author={Mohammed Baharoon and Waseem Qureshi and Jiahong Ouyang and Yanwu Xu and Abdulrhman Aljouie and Wei Peng},
      year={2023},
      eprint={2312.02366},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{oquab2023dinov2,
      title={DINOv2: Learning Robust Visual Features without Supervision}, 
      author={Maxime Oquab and TimothÃ©e Darcet and ThÃ©o Moutakanni and Huy Vo and Marc Szafraniec and Vasil Khalidov and Pierre Fernandez and Daniel Haziza and Francisco Massa and Alaaeldin El-Nouby and Mahmoud Assran and Nicolas Ballas and Wojciech Galuba and Russell Howes and Po-Yao Huang and Shang-Wen Li and Ishan Misra and Michael Rabbat and Vasu Sharma and Gabriel Synnaeve and Hu Xu and HervÃ© Jegou and Julien Mairal and Patrick Labatut and Armand Joulin and Piotr Bojanowski},
      year={2023},
      eprint={2304.07193},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
```

```text
You can read our paper explaining TabPFN [here](https://www.nature.com/articles/s41586-024-08328-6).
@article{hollmann2025tabpfn,
 title={Accurate predictions on small data with a tabular foundation model},
 author={Hollmann, Noah and M{\"u}ller, Samuel and Purucker, Lennart and
         Krishnakumar, Arjun and K{\"o}rfer, Max and Hoo, Shi Bin and
         Schirrmeister, Robin Tibor and Hutter, Frank},
 journal={Nature},
 year={2025},
 month={01},
 day={09},
 doi={10.1038/s41586-024-08328-6},
 publisher={Springer Nature},
 url={https://www.nature.com/articles/s41586-024-08328-6},
}

@inproceedings{hollmann2023tabpfn,
  title={TabPFN: A transformer that solves small tabular classification problems in a second},
  author={Hollmann, Noah and M{\"u}ller, Samuel and Eggensperger, Katharina and Hutter, Frank},
  booktitle={International Conference on Learning Representations 2023},
  year={2023}
}
```